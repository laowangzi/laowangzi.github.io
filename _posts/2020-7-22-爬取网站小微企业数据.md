---
published: true
title: 爬取网站小微企业数据
category: python
tags: 
  - crawl
  - python
layout: post

---

## 需求分析 

爬取顺企网成都火车南站所有注册资本大于100万的小企业数据。得到每个企业的企业名称、注册资本、地址、联系人、手机、固定电话。这6个属性。

## 实现 
初次接触爬虫，最先参考的一段爬取豆瓣电影排名的代码，如下：

    # --*-- coding : utf-8 --*--
    # --*-- author : silly rabbit --*--

    import requests
    from bs4 import BeautifulSoup

    class Douban:
    def __init__(self):
       self.URL = 'https://movie.douban.com/top250'
       self.starnum =[]
       for start_num in range(0,251,25):
           self.starnum.append(start_num)
       self.header = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'}

    def get_top250(self):
        for start in self.starnum:
            start = str(start)
            html = requests.get(self.URL, params={'start':start},headers = self.header)
            soup = BeautifulSoup(html.text,"html.parser")
            name = soup.select('#content > div > div.article > ol > li > div > div.info > div.hd > a > span:nth-child(1)')
            for name in name:
                print(name.get_text())

    if __name__== "__main__":
    cls = Douban()
    cls.get_top250()
	
通过这段代码，主要学习几个点：

1.requests和bs4的使用。其中requests有中文官方文档可参考。

2.如何使用请求header伪装浏览器访问请求。

3.BeautifulSoup解析html页面的常用方式html.parser

4.如何定位到你需要解析的内容`name = soup.select('#content > div > div.article > ol > li > div > div.info > div.hd > a > span:nth-child(1)')`里边的内容是在网页中选择目标内容点右键-检查-copy-copy selector得到的。

5.一个使用细节：以上一条中name变量为例，name会是list类型，当想要单纯提取text内容时，需要用一个循环访问才行，不然会报错。`for name in name:print(name.get_text())`

##思考
经过这次获取数据的过程，发现数据掺杂脏数据在所难免。在本次数据获取中，问题出在copy selector上，copy selector是写死的，但有些特殊情况在同一个网站下载到的网页的这部分内容并不是你想要的（比如本例中就出现了企业没有注册资本，那一栏写的是注册日期，结果被错误的引入了我的结果）。这种错误如果提前能预料到可能还好，比如可以把判断是否高于100万的正则表达式更改一下，但如果事后发现，重来成本太高，就只能进行数据清洗了。
