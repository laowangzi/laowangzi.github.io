---
published: true
title: 以Titanic为例学习决策树
category: python
tags: 
  - scikitlearn
  - python
  - algorithm
layout: post

---

##决策树
以

### 常见参数 ###
1.criterion:用来决定不纯度的计算方法。有两种选择

（1）entropy，信息熵

（2）gini,基尼系数

2.random_state和splitter:控制决策树的随机性。scikit-learn为了加大随机性，每次DecisionTreeClassifier开始时都随机选择一个特征做根，而不是选择信息增益最大的特征做根。该功能由random_state控制。随机选择一部分特征，选出结果最好的那颗树。

在随机森林中，就使用random_state来控制生成本不同的决策树。可以使用rcf.estimators_来查看树的情况。
### 剪枝 ##
剪枝对决策树结果影响非常大，是决策树算法调整的核心。

1.max_depth:限制最大深度，超过最大深度的都剪掉。解决过拟合问题十分实用，实际使用中建议从3开始尝试。

2.minsample_split:允许分支的最少样本数。

### 调参数之网格搜索 ##
网格搜索是一种枚举技术。计算量很大，很耗时。关键是控制好搜索范围。
     
    parameters={'criterion':('entropy','Gini')   #参数字典
                ,'max_depth':[*range(3,10)]
                }
     clf=DecisionTreeClassifier(random_state=10) #还是需要先实例化模型
     GS=GridSearch(clf,parameters,cv=10)         #网格搜索
     GS=GS.fit(Ctrain,Ytrain)
网格搜索常用接口：

1.best_params_  最佳参数

2.beast_score_ 最佳表现

网格搜索的缺陷：虽然会找到这些参数的最佳匹配，但是无法将这些参数舍弃。因此可能出现结果并不是最好的的局面。需要思考设定什么参数进去。

## 随机森林 ##

以RandomForestClassifier的bagging为例

使用bagging的两个条件：1、各个分类器尽量独立（加大随机性参数）2、基分类器正确率至少要超过随机分类器。![](D:\Git\Git-2.27.0-64-bit\repository\laowangzi.github.io\_posts\image\以Titanic为例学习决策树\p1.png)
###常见参数
1.random_state:增加生成树的随机性，设为定值则每次每棵树的random_state都不变。

2.bootstrap:增加树生成的随机性，默认为True，即鱼塘随机抽样。缺点：平均只能抽到60%的数据集，有一部分数据集会被浪费掉。因此，可以不划分训练集和测试机，用没有被抽到的数据当测试集，但这并不绝对。

3.obb_score_ :True/False，默认为false。此参数为true时代表不提前划分训练集与测试集，使用bagging中没有覆盖到的数据作为测试集。

4.n_estimators :控制森林里有几棵树。

###接口（随即森林与树相同）
1.apply：输入测试集，返回每个测试样本所在的叶子节点的索引

2.fit：训练

3.score

4.predict：预测目标数据集，返回标签。

5.feature_importance:观察特征的重要性

###用回归森林填补缺失值

scikit-learn中常用的填补缺失值的类 sklearn.impute.SimpleInputer,这个类提供了一些常用的缺失值填充方法（stratage）如平均值，众数等。

回归森林填补缺失值的思想是将要填补缺失值的属性作为target，用其他特征值train出模型预测结果。