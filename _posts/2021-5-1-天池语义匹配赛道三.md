---
published: true
title: 天池语义匹配
category: algorithm
tags: 
  - algorithm
  - competition
layout: post

---

##赛题简述
判断两句话是否为同一意思，相同为0，不同为1。与以往同类比赛有不同的是本次比赛的文本内容是脱敏处理过的，“字或词被映射为数字”，句子主要是中文，也夹杂着少量英文单词。

##最初想法：
1、观察数据正负比例大概为6.5：3.5 可能需要考虑数据长尾分布问题。

2、语料已经被转化为int型的tokens，无法通过现有的开源分词算法进行分词。因此目前放弃分词，直接把每个int当成一个词。

3、如果能找到映射表，映射回文本，重新分词跑一遍，效果可能会提升。Unicode？（投机取巧的想法，抛弃）

4、两个LSTM共享参数？我先实现分别训练，不共享参数的版本，y1和y2用距离计算相似度，双向LSTM。（孪生LSTM的思路）

##模型尝试
1、LSTM 效果不好

2、Transformer 效果不好

3、tfidf 效果不好
##最终方案
###模型部分
bert大法好，最终参考苏大佬的baseline，找了一个词典，利用词频重构确定int到text的映射，对字进行随机mask，再利用bert进行微调训练。
对baseline的改进主要是：

(1)在调参部分增大了mask的比例。

(2)加入了FGM对抗训练。

(3)更改预训练模型为Wobert。

(4)进行10折，并使用sigmoid法进行模型融合。**注意sigmoid方法，可提升融合前的信息量，参考https://blog.csdn.net/junxinwoxin/article/details/80407917**

![model](/image/text_pair/model.png)

###数据部分
1、数据截断和补充
我们将数据进行截断和补充，选择的截断阈值为32.当短文本的长度小于32时，补充0进行填充；当短文的长度大于32时，进行截断。

2、数据增强
在文本数据中，我们可以利用文本之间的传递性进行数据的增强，如A=B，B=C，则可以推出：A=C，利用这一规则进行数据的增强。通过这一手段可以增强大约20000的样本数据。

3、数据过滤
由于所有的训练样本中，数据重复的较多，这样肯定会影响模型的效果，使得模型过拟合，在这里我们进行了数据的过滤。把样本中出现次数过多的进行过滤，我们选择了一个过滤阈值为8，当文本的出现次数大于8时，进行过滤从而降低模型的过拟合程度。

##赛后优秀开源方案学习
###冠军

###第四名
